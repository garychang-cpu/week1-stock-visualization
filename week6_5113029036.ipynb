{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"UwcN41PQf73W\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# <b>Hugging Face 金融情緒分析實作</b>\\n\",\n",
        "    \"\\n\",\n",
        "    \"這個筆記本將引導您完成使用 Hugging Face `transformers` 和 `datasets` 函式庫，對金融文本進行情緒分析的完整流程。我們將涵蓋從資料載入、前處理、模型微調（Fine-tuning）到結果評估與預測的每一個步驟。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## 1. 環境設定與套件安裝\\n\",\n",
        "    \"首先，我們需要安裝 Hugging Face 提供的 `datasets` 和 `transformers` 函式庫。`datasets` 可以幫助我們輕鬆下載和處理資料集，而 `transformers` 則提供了預訓練模型和訓練工具。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"Pfwn6jilq0ro\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"!pip install datasets transformers scikit-learn pandas torch\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"pbAzKp4WM4O9\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## 2. 資料載入與前處理\\n\",\n",
        "    \"我們將使用 `FinanceInc/auditor_sentiment` 資料集，這是一個專為金融領域設計的情緒分析資料集，包含正面（Positive）、中性（Neutral）和負面（Negative）三種標籤。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"KpamGxqud9SG\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from datasets import load_dataset\\n\",\n",
        "    \"import pandas as pd\\n\",\n",
        "    \"from collections import Counter\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 從 Hugging Face Hub 載入資料集\\n\",\n",
        "    \"dataset = load_dataset(\\\"FinanceInc/auditor_sentiment\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"資料集結構：\\\")\\n\",\n",
        "    \"print(dataset)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 查看訓練集和測試集的前5筆資料\\n\",\n",
        "    \"print(\\\"\\\\n訓練集範例：\\\")\\n\",\n",
        "    \"print(dataset[\\\"train\\\"][:5])\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 標籤對應關係：{0: \\\"Negative\\\", 1: \\\"Neutral\\\", 2: \\\"Positive\\\"}\\n\",\n",
        "    \"# 分析標籤分佈，檢查是否存在類別不平衡問題\\n\",\n",
        "    \"print(\\\"\\\\n訓練集標籤分佈：\\\", Counter(dataset[\\\"train\\\"][\\\"label\\\"]))\\n\",\n",
        "    \"print(\\\"測試集標籤分佈：\\\", Counter(dataset[\\\"test\\\"][\\\"label\\\"]))\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 將資料轉換為 Pandas DataFrame 以便進行更複雜的處理\\n\",\n",
        "    \"df_train = pd.DataFrame(dataset[\\\"train\\\"])\\n\",\n",
        "    \"df_test = pd.DataFrame(dataset[\\\"test\\\"])\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 檢查並移除重複值，避免模型過擬合\\n\",\n",
        "    \"df_train.drop_duplicates(subset=[\\\"sentence\\\"], inplace=True)\\n\",\n",
        "    \"df_test.drop_duplicates(subset=[\\\"sentence\\\"], inplace=True)\\n\",\n",
        "    \"print(\\\"\\\\n移除重複值後的大小：\\\")\\n\",\n",
        "    \"print(f\\\"訓練集大小: {df_train.shape}\\\")\\n\",\n",
        "    \"print(f\\\"測試集大小: {df_test.shape}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"source\": [\n",
        "    \"## 3. 資料集劃分與 Tokenization\\n\",\n",
        "    \"接著，我們將訓練集切分為訓練集和驗證集，並使用與預訓練模型匹配的 Tokenizer 將文本轉換為模型可以理解的數字格式。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"yt-UA3uXsrD4\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from transformers import AutoTokenizer\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 將原始訓練集切分為 80% 訓練集和 20% 驗證集\\n\",\n",
        "    \"train_val_split = dataset[\\\"train\\\"].train_test_split(test_size=0.2, seed=42)\\n\",\n",
        "    \"train_dataset = train_val_split[\\\"train\\\"]\\n\",\n",
        "    \"val_dataset = train_val_split[\\\"test\\\"]\\n\",\n",
        "    \"test_dataset = dataset[\\\"test\\\"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"資料集劃分：\\\")\\n\",\n",
        "    \"print(f\\\"訓練集大小：{len(train_dataset)}\\\")\\n\",\n",
        "    \"print(f\\\"驗證集大小：{len(val_dataset)}\\\")\\n\",\n",
        "    \"print(f\\\"測試集大小：{len(test_dataset)}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# =============================================================================\\n\",\n",
        "    \"# 【作業要求 1：嘗試不同預訓練BERT模型】\\n\",\n",
        "    \"# 請修改下方的 model_name 變數來更換不同的模型。\\n\",\n",
        "    \"# 推薦嘗試的模型：\\n\",\n",
        "    \"# 1. ProsusAI/finbert (金融領域特化模型，效果最好)\\n\",\n",
        "    \"# 2. distilbert-base-uncased (輕量化模型，速度快)\\n\",\n",
        "    \"# 3. roberta-base (改進版BERT，性能更強)\\n\",\n",
        "    \"# 4. bert-base-uncased (基礎BERT模型，作為基準線)\\n\",\n",
        "    \"# =============================================================================\\n\",\n",
        "    \"model_name = \\\"ProsusAI/finbert\\\"  # 推薦使用金融領域模型\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"\\\\n使用的模型：{model_name}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 使用 AutoTokenizer 載入與模型對應的 Tokenizer\\n\",\n",
        "    \"tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 定義 Tokenization 函數\\n\",\n",
        "    \"def tokenize_function(example):\\n\",\n",
        "    \"    # padding=\\\"max_length\\\" -> 將所有句子填充到相同長度\\n\",\n",
        "    \"    # truncation=True -> 截斷超過模型最大長度的句子\\n\",\n",
        "    \"    return tokenizer(example[\\\"sentence\\\"], padding=\\\"max_length\\\", truncation=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 對所有資料集進行 Tokenization\\n\",\n",
        "    \"tokenized_train = train_dataset.map(tokenize_function, batched=True)\\n\",\n",
        "    \"tokenized_val = val_dataset.map(tokenize_function, batched=True)\\n\",\n",
        "    \"tokenized_test = test_dataset.map(tokenize_function, batched=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\nTokenization 完成！\\\")\\n\",\n",
        "    \"print(\\\"Tokenized 訓練集範例：\\\", tokenized_train[0])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"1B6hjgeKNCOY\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## 4. 模型建立與訓練\\n\",\n",
        "    \"現在我們可以載入預訓練模型，並使用 `Trainer` API 來進行微調（Fine-tuning）。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"CrZKiuOAARK0\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"import torch\\n\",\n",
        "    \"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\\n\",\n",
        "    \"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 檢查是否有可用的 GPU，若有則使用 GPU 進行訓練\\n\",\n",
        "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
        "    \"print(f\\\"使用的裝置: {device}\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 載入預訓練模型，並指定 num_labels=3 (對應三種情緒)\\n\",\n",
        "    \"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\\n\",\n",
        "    \"model.to(device) # 將模型移至 GPU\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 定義評估指標計算函數\\n\",\n",
        "    \"def compute_metrics(pred):\\n\",\n",
        "    \"    labels = pred.label_ids\\n\",\n",
        "    \"    preds = np.argmax(pred.predictions, axis=1)\\n\",\n",
        "    \"    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\\\"weighted\\\")\\n\",\n",
        "    \"    acc = accuracy_score(labels, preds)\\n\",\n",
        "    \"    return {\\n\",\n",
        "    \"        'accuracy': acc,\\n\",\n",
        "    \"        'f1': f1,\\n\",\n",
        "    \"        'precision': precision,\\n\",\n",
        "    \"        'recall': recall\\n\",\n",
        "    \"    }\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 設定訓練參數\\n\",\n",
        "    \"training_args = TrainingArguments(\\n\",\n",
        "    \"    output_dir=\\\"./results\\\",\\n\",\n",
        "    \"    eval_strategy=\\\"epoch\\\",      # 每個 epoch 結束後進行一次評估\\n\",\n",
        "    \"    learning_rate=2e-5,\\n\",\n",
        "    \"    per_device_train_batch_size=16,\\n\",\n",
        "    \"    per_device_eval_batch_size=16,\\n\",\n",
        "    \"    num_train_epochs=3,\\n\",\n",
        "    \"    weight_decay=0.01,\\n\",\n",
        "    \"    logging_dir='./logs',\\n\",\n",
        "    \"    logging_steps=10,\\n\",\n",
        "    \"    report_to=\\\"none\\\" # 關閉 wandb 等報告\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 建立 Trainer\\n\",\n",
        "    \"trainer = Trainer(\\n\",\n",
        "    \"    model=model,\\n\",\n",
        "    \"    args=training_args,\\n\",\n",
        "    \"    train_dataset=tokenized_train,\\n\",\n",
        "    \"    eval_dataset=tokenized_val,\\n\",\n",
        "    \"    compute_metrics=compute_metrics,\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 開始訓練\\n\",\n",
        "    \"trainer.train()\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"O2CSuye8NPbm\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## 5. 結果評估\\n\",\n",
        "    \"訓練完成後，我們在驗證集和從未見過的測試集上評估模型的最終表現。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"QUKztZcotkI7\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# 評估模型在驗證集的表現\\n\",\n",
        "    \"print(\\\"\\\\n=== 驗證集評估結果 ===\\\")\\n\",\n",
        "    \"val_result = trainer.evaluate(eval_dataset=tokenized_val)\\n\",\n",
        "    \"print(val_result)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 評估模型在測試集的最終表現\\n\",\n",
        "    \"print(\\\"\\\\n=== 測試集評估結果 (最終成績) ===\\\")\\n\",\n",
        "    \"test_result = trainer.evaluate(eval_dataset=tokenized_test)\\n\",\n",
        "    \"print(test_result)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"PSgCkDKhwF_o\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## 6. 自行選擇句子進行預測\\n\",\n",
        "    \"最後，我們可以載入訓練好的模型，並對自己選擇的句子進行情緒預測。\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"V5ypsiDh1ACF\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# 先展示測試集中的句子，方便您選擇\\n\",\n",
        "    \"print(\\\"=== 測試集句子展示 ===\\\")\\n\",\n",
        "    \"print(\\\"以下是測試集中的前 50 個句子：\\\\n\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"label_map = {0: \\\"Negative\\\", 1: \\\"Neutral\\\", 2: \\\"Positive\\\"}\\n\",\n",
        "    \"\\n\",\n",
        "    \"for i in range(50):\\n\",\n",
        "    \"    sentence = test_dataset[i][\\\"sentence\\\"]\\n\",\n",
        "    \"    label = test_dataset[i][\\\"label\\\"]\\n\",\n",
        "    \"    print(f\\\"[{i}] {sentence}\\\")\\n\",\n",
        "    \"    print(f\\\"    真實標籤：{label_map[label]}\\\\n\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"ZmWdctbTwO_f\"\n",
        "   },\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# =============================================================================\\n\",\n",
        "    \"# 【作業要求 2：自行選擇測試句子進行預測】\\n\",\n",
        "    \"# 請從上方顯示的測試集句子中，選擇 5 個你感興趣的句子索引填入下方列表。\\n\",\n",
        "    \"# 建議可以選擇不同情緒的句子，或是看起來比較模稜兩可的句子來測試模型。\\n\",\n",
        "    \"# =============================================================================\\n\",\n",
        "    \"test_indices = [13, 26, 36, 43, 48]  # 請修改這裡！\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"\\\\n您選擇的測試句子索引：{test_indices}\\\")\\n\",\n",
        "    \"# =============================================================================\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 根據選擇的索引，取得句子和真實標籤\\n\",\n",
        "    \"test_texts = [test_dataset[i][\\\"sentence\\\"] for i in test_indices]\\n\",\n",
        "    \"true_labels = [test_dataset[i][\\\"label\\\"] for i in test_indices]\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n=== 開始預測 ===\\\\n\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 使用 pipeline 可以更方便地進行預測\\n\",\n",
        "    \"from transformers import pipeline\\n\",\n",
        "    \"\\n\",\n",
        "    \"classifier = pipeline(\\\"sentiment-analysis\\\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\\n\",\n",
        "    \"predictions = classifier(test_texts)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Hugging Face pipeline 的標籤可能是 LABEL_0, LABEL_1, ... 需要對應回我們的標籤\\n\",\n",
        "    \"pipe_label_map = {\\\"LABEL_0\\\": \\\"Negative\\\", \\\"LABEL_1\\\": \\\"Neutral\\\", \\\"LABEL_2\\\": \\\"Positive\\\"}\\n\",\n",
        "    \"\\n\",\n",
        "    \"correct_count = 0\\n\",\n",
        "    \"for i, idx in enumerate(test_indices):\\n\",\n",
        "    \"    text = test_texts[i]\\n\",\n",
        "    \"    true_label_text = label_map[true_labels[i]]\\n\",\n",
        "    \"    pred_label_text = pipe_label_map[predictions[i]['label']]\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    is_correct = (true_label_text == pred_label_text)\\n\",\n",
        "    \"    if is_correct:\\n\",\n",
        "    \"        correct_count += 1\\n\",\n",
        "    \"    correct_mark = \\\"✓ 正確\\\" if is_correct else \\\"✗ 錯誤\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"    print(f\\\"{i+1}. 測試集索引 [{idx}] {correct_mark}\\\")\\n\",\n",
        "    \"    print(f\\\"   句子：{text}\\\")\\n\",\n",
        "    \"    print(f\\\"   真實標籤：{true_label_text}\\\")\\n\",\n",
        "    \"    print(f\\\"   預測標籤：{pred_label_text} (信心分數: {predictions[i]['score']:.4f})\\\\n\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"accuracy = (correct_count / len(test_indices)) * 100\\n\",\n",
        "    \"print(f\\\"預測準確率：{correct_count}/{len(test_indices)} = {accuracy:.2f}%\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"\\\\n【作業要求 3：使用大語言模型進行情緒分類】\\\")\\n\",\n",
        "    \"print(\\\"對於上方預測標示為 '✗ 錯誤' 的句子，請參考 'LLM情緒分類比較.md' 檔案的說明，\\\")\\n\",\n",
        "    \"print(\\\"嘗試使用 Zero-shot 和 Few-shot prompting 讓大語言模型進行預測，並比較結果。\\\")\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"colab\": {\n",
        "   \"gpuType\": \"T4\",\n",
        "   \"provenance\": []\n",
        "  },\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"name\": \"python\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 0\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "s0JMnejBsV_s"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}